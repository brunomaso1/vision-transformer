{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\"><b> Ejemplo de fine-tuning </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "\n",
    "<!-- [![Binder](http://mybinder.org/badge.svg)](https://mybinder.org/) -->\n",
    "<!-- [![nbviewer](https://img.shields.io/badge/render-nbviewer-orange?logo=Jupyter)](https://nbviewer.org/)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/) -->\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "/* Limitar la altura de las celdas de salida en html */\n",
    ".jp-OutputArea.jp-Cell-outputArea {\n",
    "    max-height: 500px;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üõª <em><font color='MediumSeaGreen'>  Instalaciones: </font></em> üõª\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook utiliza [Poetry](https://python-poetry.org/) para la gesti√≥n de dependencias.\n",
    "Primero instala Poetry siguiendo las instrucciones de su [documentaci√≥n oficial](https://python-poetry.org/docs/#installation).\n",
    "Luego ejecuta el siguiente comando para instalar las dependencias necesarias y activar el entorno virtual:\n",
    "\n",
    "- Bash:\n",
    "\n",
    "```bash\n",
    "poetry install\n",
    "eval $(poetry env activate)\n",
    "```\n",
    "\n",
    "- PowerShell:\n",
    "\n",
    "```powershell\n",
    "poetry install\n",
    "Invoke-Expression (poetry env activate)\n",
    "```\n",
    "\n",
    "> üìù <em><font color='Gray'>Nota:</font></em> Para agregar `pytorch` utilizando Poetry, se utiliza el siguiente comando:\n",
    "> ```bash\n",
    "> # M√°s info: https://github.com/python-poetry/poetry/issues/6409\n",
    "> poetry source add --priority explicit pytorch_gpu https://download.pytorch.org/whl/cu128 # Seleccionar la wheel adecuada para tu GPU\n",
    "> poetry add --source pytorch_gpu torch torchvision \n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚úã <em><font color='DodgerBlue'>Importaciones:</font></em> ‚úã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-18 19:34:28.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvision_transformer.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1mPROJ_ROOT path is: E:\\Documentos\\Git Repositories\\vision-transformer\u001b[0m\n",
      "\u001b[32m2025-06-18 19:34:28.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvision_transformer.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m19\u001b[0m - \u001b[1mActual environment is: dev\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Recarga autom√°tica de m√≥dulos en Jupyter Notebook\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from datasets import Dataset\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import mlflow\n",
    "import requests\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Modulos propios\n",
    "from vision_transformer.dataset import load_huggingface_dataset\n",
    "from vision_transformer.config import MLFLOW_URL, MODELS_DIR, PREFECT_URL, DATASET_NAME, DATASET_VERSION, HISTORY_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîß <em><font color='tomato'>Configuraciones:</font></em> üîß\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo actual: cuda\n",
      "\u001b[32m2025-06-18 19:34:31.240\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m17\u001b[0m - \u001b[32m\u001b[1mConexi√≥n a MLflow establecida correctamente.\u001b[0m\n",
      "\u001b[32m2025-06-18 19:34:33.261\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m29\u001b[0m - \u001b[32m\u001b[1mConexi√≥n a Prefect establecida correctamente.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16 \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Establece el dispositivo.\n",
    "print(f\"Dispositivo actual: {DEVICE}\")\n",
    "# # torch.set_float32_matmul_precision('highest') # Optimizaci√≥n: Establece la precisi√≥n de las multiplicaciones de matrices de punto flotante de 32 bits en 'm√°s alta'.\n",
    "# torch.set_float32_matmul_precision('high') # Optimizaci√≥n: Establece la precisi√≥n de las multiplicaciones de matrices de punto flotante de 32 bits en 'alta'.\n",
    "# # torch.set_float32_matmul_precision('medium') # Optimizaci√≥n: Establece la precisi√≥n de las multiplicaciones de matrices de punto flotante de 32 bits en 'media'.\n",
    "# # torch.backends.cudnn.benchmark = True # Optimizaci√≥n: Para redes CNN (pero como se usa una capa convolucional, se establece en True).\n",
    "\n",
    "MODEL_NAME = \"vit-base\"  # Nombre del modelo de Hugging Face.\n",
    "MODEL_FOLDER = MODELS_DIR / \"vit-base\"\n",
    "CHECKPOINT = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# MLflow: Configuraci√≥n de la URI de seguimiento\n",
    "try:\n",
    "    response = requests.get(MLFLOW_URL)\n",
    "    response.raise_for_status()  # Verifica si la solicitud fue exitosa.\n",
    "    logger.success(\"Conexi√≥n a MLflow establecida correctamente.\")\n",
    "    os.environ[\"MLFLOW_TRACKING_URI\"] = MLFLOW_URL  # Configura la URI de seguimiento de MLflow.\n",
    "    os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = CHECKPOINT.replace(\"/\", \"_\")  # Configura el nombre del experimento de MLflow.\n",
    "    os.environ[\"MLFLOW_TAGS\"] = '{\"model_family\": \"swinv2\"}'\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error al conectar con MLflow. Tienes levantado el servidor de MLflow?\")\n",
    "    raise SystemExit(f\"Error al conectar con MLflow: {e}\")\n",
    "\n",
    "# Prefect: Configuraci√≥n de Prefect\n",
    "try:\n",
    "    response = requests.get(PREFECT_URL)\n",
    "    response.raise_for_status()  # Verifica si la solicitud fue exitosa.\n",
    "    logger.success(\"Conexi√≥n a Prefect establecida correctamente.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error al conectar con Prefect. Tienes levantado el servidor de Prefect?\")\n",
    "    raise SystemExit(f\"Error al conectar con Prefect: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">‚ú®Datos del proyecto:‚ú®</div>\n",
    "\n",
    "<p></p>\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Subtitulo       | *Fine-tuning* del modelo swimv2 sobre el dataset EuroSAT                                                                       |\n",
    "| --------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Descrpci√≥n**  | <small>An√°lisis exploratorio del proceso de *fine-tuning* del swimv2 sobre el EuroSAT<br/>- *Tarea:* `Clasificaci√≥n`<br/>- *Modelo*: `swimv2`<br/> - *Dataset*: `EuroSAT` </small>|\n",
    "| **Autor** | <small>[Nombre] ([correo]) </small>                                                                                                 |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de contenidos\n",
    "1. [Carga de datos](#carga-de-datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga de datos <a name=\"carga-de-datos\"></a>\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-18 19:34:33.303\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvision_transformer.dataset\u001b[0m:\u001b[36mload_huggingface_dataset\u001b[0m:\u001b[36m441\u001b[0m - \u001b[1mCargando el dataset procesado...\u001b[0m\n",
      "\u001b[32m2025-06-18 19:34:33.588\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mvision_transformer.dataset\u001b[0m:\u001b[36mload_huggingface_dataset\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mEl dataset contiene m√∫ltiples conjuntos (train, test, val). Cargando todos...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f937c318c0e54b2698d62c4ea79bd557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/24300 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b911d82df54e1c8c0e09d440d34465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/2700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3df1caf1804347b4878d4b19e5781eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/24300 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f8b899ee5e40e8a38737ce8b14869d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0/2700 [00:00<?, ?files/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff88eb6ad3d444dfbf44b2b3260db052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a447838e1031464bb53e7882797ba9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_huggingface_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 24300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 2700\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Procesamiento de datos <a name=\"procesamiento-de-datos\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[\"train\"].features[\"label\"].names\n",
    "id2label = {id: label for id, label in enumerate(dataset[\"train\"].features[\"label\"].names)}\n",
    "label2id = {label: id for id, label in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Comentar este bloque de prueba cuando se use todo el dataset.\n",
    "from vision_transformer.config import RANDOM_SEED\n",
    "\n",
    "dataset['train'] = dataset['train'].shuffle(seed=RANDOM_SEED).select(range(100))\n",
    "dataset['test'] = dataset['test'].shuffle(seed=RANDOM_SEED).select(range(80))\n",
    "test_image = dataset['test'][0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=64x64 at 0x1FD707D8410>, 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0, 'pixel_values': tensor([[[ 0.6863,  0.6863,  0.6863,  ...,  0.2000,  0.2000,  0.2000],\n",
      "         [ 0.6863,  0.6863,  0.6863,  ...,  0.2000,  0.2000,  0.2000],\n",
      "         [ 0.6863,  0.6863,  0.6863,  ...,  0.2000,  0.2000,  0.2000],\n",
      "         ...,\n",
      "         [ 0.7569,  0.7569,  0.7569,  ...,  0.1922,  0.1922,  0.1922],\n",
      "         [ 0.7569,  0.7569,  0.7569,  ...,  0.1922,  0.1922,  0.1922],\n",
      "         [ 0.7569,  0.7569,  0.7569,  ...,  0.1922,  0.1922,  0.1922]],\n",
      "\n",
      "        [[ 0.4588,  0.4588,  0.4588,  ..., -0.0118, -0.0118, -0.0118],\n",
      "         [ 0.4588,  0.4588,  0.4588,  ..., -0.0118, -0.0118, -0.0118],\n",
      "         [ 0.4588,  0.4588,  0.4588,  ..., -0.0118, -0.0118, -0.0118],\n",
      "         ...,\n",
      "         [ 0.4510,  0.4510,  0.4510,  ...,  0.0588,  0.0588,  0.0588],\n",
      "         [ 0.4510,  0.4510,  0.4510,  ...,  0.0588,  0.0588,  0.0588],\n",
      "         [ 0.4510,  0.4510,  0.4510,  ...,  0.0588,  0.0588,  0.0588]],\n",
      "\n",
      "        [[ 0.3490,  0.3490,  0.3490,  ..., -0.0667, -0.0667, -0.0667],\n",
      "         [ 0.3490,  0.3490,  0.3490,  ..., -0.0667, -0.0667, -0.0667],\n",
      "         [ 0.3490,  0.3490,  0.3490,  ..., -0.0667, -0.0667, -0.0667],\n",
      "         ...,\n",
      "         [ 0.3333,  0.3333,  0.3333,  ..., -0.0039, -0.0039, -0.0039],\n",
      "         [ 0.3333,  0.3333,  0.3333,  ..., -0.0039, -0.0039, -0.0039],\n",
      "         [ 0.3333,  0.3333,  0.3333,  ..., -0.0039, -0.0039, -0.0039]]])}\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])\n",
    "print(dataset['train'][0]['pixel_values'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IMPORTANTE: Si se est√° en un .py, se debe ejecutar dentro del bloque `if __name__ == \"__main__\":`\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     benchmark_dataloader(...)\n",
    "\n",
    "import multiprocessing\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "from datasets import load_dataset  # solo por referencia, ya lo ten√©s cargado\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "simulation_dataset = copy.deepcopy(dataset['train'])\n",
    "\n",
    "# Asegurarse de que est√© en formato PyTorch\n",
    "simulation_dataset.set_format(type=\"torch\", columns=[\"image\", \"label\"])\n",
    "\n",
    "CPU_COUNT = multiprocessing.cpu_count()\n",
    "\n",
    "def benchmark_dataloader(dataset, batch_size=BATCH_SIZE, cpu_count=CPU_COUNT):\n",
    "    num_workers_list = [i for i in range(0, cpu_count, 2)]  # De 0 a CPU_COUNT en pasos de 2\n",
    "    logger.info(f\"üß™ Benchmark con {CPU_COUNT} CPUs l√≥gicos\")\n",
    "    for num_workers in num_workers_list:\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "        start = time.time()\n",
    "        for _ in tqdm(loader, desc=f\"num_workers={num_workers:>2}\", leave=False):\n",
    "            pass  # Solo iteramos para medir el tiempo\n",
    "        end = time.time()\n",
    "        \n",
    "        logger.info(f\"üîπ num_workers = {num_workers:>2} ‚Üí tiempo = {end - start:.2f} s\")\n",
    "\n",
    "\n",
    "# # Ejecutar benchmark\n",
    "# # benchmark_dataloader(simulation_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback, TrainerCallback\n",
    "from transformers.integrations import MLflowCallback\n",
    "\n",
    "# mlflowcallback = MLflowCallback(\n",
    "#     tracking_uri=\"http://localhost:5000\",  # Cambia esto si tu servidor MLflow est√° en otro lugar.\n",
    "#     experiment_name=\"vision_transformer_experiment\",\n",
    "#     save_model=False,  # No guardar el modelo autom√°ticamente, se guardar√° manualmente.\n",
    "# )\n",
    "\n",
    "\n",
    "class EmptyCacheCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "callback_list = [\n",
    "    EarlyStoppingCallback(\n",
    "        early_stopping_patience=2\n",
    "    ),  # Usar con: metric_for_best_model = \"eval_accuracy\" o \"eval_loss\" en Trainer.\n",
    "    MLflowCallback()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViTForImageClassification(\n",
      "  (vit): ViTModel(\n",
      "    (embeddings): ViTEmbeddings(\n",
      "      (patch_embeddings): ViTPatchEmbeddings(\n",
      "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): ViTEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x ViTLayer(\n",
      "          (attention): ViTAttention(\n",
      "            (attention): ViTSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (output): ViTSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): ViTIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): ViTOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  )\n",
      "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "from vision_transformer.config import MODELS_DIR\n",
    "\n",
    "# NOTA: Un \"step\" es un \"batch\".\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODELS_DIR / MODEL_FOLDER,  # Directorio de salida para los modelos entrenados.\n",
    "    overwrite_output_dir=True,  # Sobrescribe el directorio de salida si ya existe.\n",
    "    eval_strategy=\"epoch\",  # Estrategia de evaluaci√≥n: eval√∫a al final de cada √©poca.\n",
    "    per_device_train_batch_size=BATCH_SIZE, # Tama√±o del lote por dispositivo durante el entrenamiento. Oportunidad de optimizaci√≥n.\n",
    "    per_device_eval_batch_size=BATCH_SIZE, # Tama√±o del lote por dispositivo durante la evaluaci√≥n. Oportunidad de optimizaci√≥n.\n",
    "    # gradient_accumulation_steps=4,    # Acumulaci√≥n de gradientes: acumula gradientes antes de hacer un optimizer.step() para simular un tama√±o de lote m√°s grande. Por defecto es 1.\n",
    "                                        # Oportunidad de optimizaci√≥n.\n",
    "    # eval_accumulation_steps=4, # Acumulaci√≥n de gradientes durante la evaluaci√≥n: Sin establecer, acumula todas las predicciones antes de calcular las m√©tricas (consume mas memoria). \n",
    "                                 # Oportunidad de optimizaci√≥n.\n",
    "    # eval_delay=10, # Retraso de evaluaci√≥n: espera 10 pasos antes de realizar la primera evaluaci√≥n.\n",
    "    # torch_empty_cache_steps=len(dataset['train']) // BATCH_SIZE   # Pasos para vaciar la cach√© de PyTorch: vac√≠a la cach√© cada vez que se completa un paso de entrenamiento.\n",
    "                                                                    # Tambi√©n se puede utilizar un callback.\n",
    "    learning_rate=5e-5,  # Tasa de aprendizaje: tasa de aprendizaje inicial para el optimizador AdamW | Si se utiliza scheduler, puede ser mayor (por ejemplo, 1e-4, 2e-4 o 1e-3).\n",
    "                         # Oportunidad de optimizaci√≥n.\n",
    "    # weight_decay=0.01,  # Decaimiento del peso: regularizaci√≥n L2 para evitar el sobreajuste (se aplica a todos menos a los bias y capas de normalizaci√≥n).\n",
    "                        # Oportunidad de optimizaci√≥n.\n",
    "    # adam_beta1=0.9, # Beta1 para el optimizador AdamW: primer momento. Oportunidad de optimizaci√≥n.\n",
    "    # adam_beta2=0.999, # Beta2 para el optimizador AdamW: segundo momento. Oportunidad de optimizaci√≥n.\n",
    "    # adam_epsilon=1e-8, # Epsilon para el optimizador AdamW: evita la divisi√≥n por cero. Oportunidad de optimizaci√≥n.\n",
    "    num_train_epochs=5,  # N√∫mero de √©pocas de entrenamiento: n√∫mero total de √©pocas para entrenar el modelo.\n",
    "    # lr_scheduler_type=\"reduce_lr_on_plateau\", # Decaimiento de la tasa de aprendizaje: reduce la tasa de aprendizaje cuando la m√©trica de evaluaci√≥n no mejora. \n",
    "                                                # Se puede user \"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\".\n",
    "                                                # \"cosine_with_restarts\" es √∫til para ciclos de entrenamiento, cuando se estanca en un m√≠nimo local (o suponemos que se estanca).\n",
    "                                                # Oportunidad de optimizaci√≥n.\n",
    "    # lr_scheduler_kwargs={\n",
    "    #     \"factor\": 0.1,\n",
    "    #     \"patience\": 10,\n",
    "    #     \"threshold\": 1e-4,\n",
    "    #     \"min_lr\": 0,\n",
    "    #     \"mode\": \"min\",\n",
    "    #     \"verbose\": True\n",
    "    # }, # Argumentos adicionales para el planificador de tasa de aprendizaje. Oportunidad de optimizaci√≥n.\n",
    "    warmup_ratio=0.1,   # Proporci√≥n de calentamiento: aumenta la tasa de aprendizaje linealmente desde 0 hasta la tasa \n",
    "                          # de aprendizaje inicial durante el porcentaje especificado del total\n",
    "                          # de pasos de entrenamiento (por ejemplo, 0.1 significa que la tasa de aprendizaje se calentar√° durante el 10% de los pasos de entrenamiento. \n",
    "                          # Ej: 10 √©pocas con 100 batches (o sea 1000 pasos), la tasa de aprendizaje se calentar√° durante los primeros 100 pasos (10% de 1000)). \n",
    "                          # NO APLICA PARA \"reduce_lr_on_plateau\" o \"constant\". Oportunidad de optimizaci√≥n.\n",
    "    # log_level=\"info\", # Nivel de registro: establece el nivel de registro para el entrenamiento \n",
    "                        # (puede ser \"debug\", \"info\", \"warning\", \"error\" o \"critical\"). Por defecto es \"passive\"\n",
    "                        # que loguea lo actual de la librer√≠a transformers (\"warning\")\n",
    "    # logging_steps=10,   # Pasos de registro: n√∫mero de pasos entre registros. Si es 10, se registra cada 10 pasos.\n",
    "    save_strategy=\"best\", # Estrategia de guardado: puede ser \"no\", \"epoch\", \"steps\" o \"best\". Si es \"steps\", se guarda cada cierto n√∫mero de pasos definido en save_steps.\n",
    "    save_total_limit=1, # Limite de guardado total: n√∫mero m√°ximo de modelos guardados. Si se supera, se eliminan los m√°s antiguos.\n",
    "    logging_strategy=\"epoch\",  # Estrategia de registro: puede ser \"no\", \"epoch\" o \"steps\". Si es \"steps\", se registra cada cierto n√∫mero de pasos definido en logging_steps.\n",
    "    seed=RANDOM_SEED,  # Semilla para la reproducibilidad: establece una semilla para la generaci√≥n de n√∫meros aleatorios.\n",
    "    # bf16=True,    # Habilita el entrenamiento en punto flotante de 16 bits (FP16): reduce el uso de memoria y acelera el \n",
    "                    # entrenamiento en GPUs compatibles (Ampere o posteriores). Sino utiliza 32.\n",
    "    # fp16=True, # Habilita el entrenamiento en punto flotante de 16 bits (FP16): reduce el uso de memoria y acelera el \n",
    "                 # entrenamiento en GPUs compatibles (Volta o posteriores). Sino utiliza 32.\n",
    "    # tf32=True, # Habilita el entrenamiento en punto flotante de 32 bits (TF32): acelera el entrenamiento en \n",
    "                 # GPUs Ampere o posteriores. Relacionado con matmul de PyTorch.\n",
    "    # dataloader_num_workers=0,  # N√∫mero de trabajadores para el cargador de datos: n√∫mero de procesos que se utilizan para cargar los datos en paralelo. \n",
    "                               # Por defecto 0 (thread principal). NOTA: En notebook, hay que usar 0. En .py, se puede cambiar, pero se debe poner\n",
    "                               # el c√≥digo de entrenamiento dentro de un bloque `if __name__ == \"__main__\":` para evitar problemas de multiproceso.\n",
    "                               # Esto en Windows.\n",
    "    # run_name=\"vision_transformer_experiment\", # Nombre de la ejecuci√≥n: nombre de la ejecuci√≥n para el seguimiento de experimentos (MLflow, WandB, etc.). \n",
    "                                                # Si no se especifica, se usa output_dir.\n",
    "    remove_unused_columns=False,  # Elimina las columnas no utilizadas: si es True, elimina las columnas que no se utilizan en el modelo. \n",
    "                                  # Si es False, conserva todas las columnas.\n",
    "                                  # Para este caso debe ser obligatoriamente False, ya que se usa una columna personalizada \"pixel_values\" para las im√°genes.\n",
    "    load_best_model_at_end=True,  # Carga el mejor modelo al final del entrenamiento: carga el modelo con la mejor m√©trica de evaluaci√≥n al final del entrenamiento.\n",
    "    metric_for_best_model=\"accuracy\",   # M√©trica para el mejor modelo: m√©trica que se utiliza para determinar el mejor modelo. \n",
    "                                        # Debe ser el nombre de una m√©trica que retorne el \"evaluator\".\n",
    "                                        # Se usa en conjunto con `load_best_model_at_end=True`\n",
    "    # optim=\"adamw_torch\",  # Optimizador: optimizador a utilizar. Puede ser \"adamw_torch\", \"adamw_hf\" o \"sgd\". Por defecto es \"adamw_torch\".\n",
    "                            # Lista completa: https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py#L143\n",
    "    # optim_kwargs={} # Argumentos adicionales para el optimizador: diccionario con argumentos adicionales para el optimizador.\n",
    "    # dataloader_persistent_workers=True,  # Habilita trabajadores persistentes para el cargador de datos: si es True, los trabajadores se mantienen \n",
    "                                         # activos entre √©pocas (consume m√°s memoria RAM). Por defecto es False.\n",
    "    # dataloader_prefetch_factor=2,   # Factor de prefetch para el cargador de datos: n√∫mero de lotes que se prefetchean en segundo plano. \n",
    "                                    # Si es 2, significa que habr√° 2 * num_workers lotes \n",
    "    # auto_find_batch_size=True,  # Encuentra autom√°ticamente el tama√±o de lote: si es True, busca autom√°ticamente el tama√±o de lote m√°s \n",
    "                                # grande que se puede utilizar sin exceder la memoria GPU.\n",
    "                                # Hay que tener accelerate instalado. Por defecto False.\n",
    "    # torch_compile=True, # Optimizaci√≥n: Habilita la compilaci√≥n de PyTorch para acelerar el entrenamiento.\n",
    "    # torch_compile_backend=\"max-autotune\", # Optimizaci√≥n: Utiliza la compilaci√≥n de PyTorch para acelerar el entrenamiento.\n",
    "    # torch_compile_mode=\"default\",\n",
    "    # neftune_noise_alpha=0.1, # Optimizaci√≥n: Ajusta el ruido de Neftune.\n",
    "    # eval_on_start=True,  # Eval√∫a al inicio del entrenamiento: si es True, eval√∫a el modelo al inicio del entrenamiento (prueba que funcione correctamente).\n",
    "    # use_liger_kernel=True # Optimizaci√≥n: Utiliza el kernel Liger para acelerar el entrenamiento.\n",
    "    report_to=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,    # Modelo a entrenar. Puede ser uno preentrenado o uno personalizado (torch.nn.Module). \n",
    "                    # Si no se especifica, se debe usar `model_init` para inicializar el modelo.\n",
    "    args=training_args, # Argumentos de entrenamiento: TrainingArguments que configuran el entrenamiento.\n",
    "    data_collator=data_collator,    # Data collator: collator que se utiliza para agrupar los datos en lotes a partir de una\n",
    "                                    # lista de elementos de train_dataset o eval_dataset. Por defecto es DefaultDataCollator,\n",
    "                                    # si no se pasa processing_class.\n",
    "    train_dataset=dataset['train'],  # Dataset de entrenamiento: dataset que se utiliza para el entrenamiento.\n",
    "    eval_dataset=dataset['test'],    # Dataset de evaluaci√≥n: dataset que se utiliza para la evaluaci√≥n.\n",
    "    processing_class=image_processor,  # Procesador de im√°genes: procesador que se utiliza para pre-procesar las im√°genes antes de pasarlas al modelo.\n",
    "    # model_init=None,  # Inicializador de modelo: funci√≥n que se utiliza para inicializar el modelo si no se pasa un modelo preentrenado. \n",
    "                        # Cada vez que se llama a `trainer.train()`, se inicializa un nuevo modelo (una nueva instancia). \n",
    "                        # Se suele utilizar para optimizaci√≥n. Puede tener un argumento que contiene un trial de Optuna o \n",
    "                        # Ray Tune, por ejemplo, para optimizar hiperpar√°metros. Esto permite modificar la arquitectura.\n",
    "    # compute_loss_func=None, # Funci√≥n de p√©rdida personalizada: funci√≥n que se utiliza para calcular la p√©rdida durante el entrenamiento.\n",
    "    compute_metrics=compute_metrics,    # Funci√≥n de m√©tricas personalizada: funci√≥n que se utiliza para calcular las m√©tricas durante la evaluaci√≥n.\n",
    "                                        # Debe recibir un objeto `EvalPrediction` y retornar un diccionario con las m√©tricas calculadas.\n",
    "    callbacks=callback_list,  # Lista de callbacks: lista de callbacks que se ejecutan durante el entrenamiento.\n",
    "    # optimizers=(None, None),  # Optimizador y scheduler: tuplas que contienen el optimizador y el scheduler a utilizar. Por defecto AdamW.\n",
    "    # optimizer_cls_and_kwargs=None,  # Clase y argumentos del optimizador: tupla que contiene la clase del optimizador y un \n",
    "                                    # diccionario con los argumentos a pasar al optimizador. Sobreescribe `optim` \n",
    "                                    # y `optim_kwargs` de `TrainingArguments`.\n",
    "    # preprocess_logits_for_metrics=None,   # Preprocesador de logits para m√©tricas: funci√≥n que se utiliza para preprocesar los logits \n",
    "                                            # antes de calcular las m√©tricas.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run adventurous-bear-539 at: http://localhost:8080/#/experiments/7/runs/838d9d99b3514894bf3f546555c6d924\n",
      "üß™ View experiment at: http://localhost:8080/#/experiments/7\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'transforms_to_string'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# if __name__ == \"__main__\": # NOTA IMPORTANTE: Para cambiar el n√∫mero de workers, se debe hacerlo en un .py y en el bloque `if __name__ == \"__main__\":`\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# sino explota en Windows. Esto pasa en un notebook por ejemplo. En notebook dejar el n√∫mero de workers en 0.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m mlflow.start_run():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     mlflow.log_param(\u001b[33m\"\u001b[39m\u001b[33mtransforms\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransforms_to_string\u001b[49m())\n\u001b[32m      5\u001b[39m     mlflow.log_param(\u001b[33m\"\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m\"\u001b[39m, DATASET_NAME)\n\u001b[32m      6\u001b[39m     mlflow.log_param(\u001b[33m\"\u001b[39m\u001b[33mdataset_version\u001b[39m\u001b[33m\"\u001b[39m, DATASET_VERSION)\n",
      "\u001b[31mAttributeError\u001b[39m: 'function' object has no attribute 'transforms_to_string'"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\": # NOTA IMPORTANTE: Para cambiar el n√∫mero de workers, se debe hacerlo en un .py y en el bloque `if __name__ == \"__main__\":`\n",
    "# sino explota en Windows. Esto pasa en un notebook por ejemplo. En notebook dejar el n√∫mero de workers en 0.\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"dataset_name\", DATASET_NAME)\n",
    "    mlflow.log_param(\"dataset_version\", DATASET_VERSION)\n",
    "\n",
    "    logger.info(\"Iniciando entrenamiento del modelo...\")\n",
    "    trainer.train()\n",
    "    logger.info(\"Entrenamiento finalizado. Guardando el modelo...\")\n",
    "\n",
    "history = pd.DataFrame(trainer.state.log_history)\n",
    "history.to_csv(MODELS_DIR / MODEL_FOLDER / HISTORY_FILENAME, index=False)\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'Forest', 'score': 0.11400282382965088},\n",
       " {'label': 'SeaLake', 'score': 0.10881276428699493},\n",
       " {'label': 'AnnualCrop', 'score': 0.10688897967338562},\n",
       " {'label': 'PermanentCrop', 'score': 0.10486926883459091},\n",
       " {'label': 'River', 'score': 0.10069800168275833}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicci√≥n con pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('image-classification', model=model, image_processor=image_processor, device=DEVICE)\n",
    "classifier(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: Forest (index: 1)\n"
     ]
    }
   ],
   "source": [
    "# Predicci√≥n con Pytorch\n",
    "inputs = image_processor(test_image, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs.to(DEVICE)).logits\n",
    "\n",
    "predicted_label_index = logits.argmax(-1).item()\n",
    "predicted_label = model.config.id2label[predicted_label_index]\n",
    "print(f\"Predicted label: {predicted_label} (index: {predicted_label_index})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-transformer-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
